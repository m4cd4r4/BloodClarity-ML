% BloodClarity Research Paper - BibTeX Citation File
% Version 1.0 - December 14, 2025

% Main Paper Citation
@article{bloodclarity2025clinical,
  title={Clinical-Grade Browser-Based Machine Learning for Medical Biomarker Extraction from Laboratory Reports},
  author={{Macdara Ó Murchú}},
  journal={Preprint},
  year={2025},
  month={December},
  volume={},
  number={},
  pages={},
  doi={},
  note={Version 1.0},
  url={https://github.com/m4cd4r4/bloodclarity/docs/paper},
  keywords={Medical Informatics, Named Entity Recognition, Browser-Based Machine Learning, Clinical Document Processing, HIPAA Compliance, WebGPU, ONNX, TinyBERT, Multi-Task Learning, Model Compression},
  abstract={Clinical laboratory report interpretation remains a significant challenge in medical informatics. We present a high-accuracy (98.8% extraction accuracy on validated samples) biomarker extraction system deployable entirely in-browser with complete offline capability. Our five-component system combines synthetic training data generation (10,000 samples across 16 formats and 102 biomarkers), multi-task learning architecture, OCR preprocessing, biological plausibility validation, and context-aware unit conversion. The optimized 12MB model achieves 45-80ms inference latency, representing a 10× speed improvement over cloud-based alternatives while eliminating $162,000 in annual costs.}
}

% Alternative citation formats for different venues

% Conference Paper Format (for AMIA, ICML, etc.)
@inproceedings{bloodclarity2025clinical_conf,
  title={Clinical-Grade Browser-Based Machine Learning for Medical Biomarker Extraction from Laboratory Reports},
  author={{Macdara Ó Murchú}},
  booktitle={Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium},
  year={2026},
  month={November},
  pages={},
  organization={AMIA},
  address={San Francisco, CA},
  note={Submitted for review}
}

% Journal Article Format (for JAMIA, JBI, etc.)
@article{bloodclarity2025clinical_journal,
  title={Clinical-Grade Browser-Based Machine Learning for Medical Biomarker Extraction from Laboratory Reports},
  author={{Macdara Ó Murchú}},
  journal={Journal of the American Medical Informatics Association},
  year={2026},
  volume={},
  number={},
  pages={},
  publisher={Oxford University Press},
  doi={},
  issn={1527-974X},
  note={Submitted for review}
}

% Technical Report Format
@techreport{bloodclarity2025clinical_tech,
  title={Clinical-Grade Browser-Based Machine Learning for Medical Biomarker Extraction from Laboratory Reports},
  author={{Macdara Ó Murchú}},
  institution={BloodClarity Project},
  year={2025},
  month={December},
  type={Technical Report},
  number={TR-2025-01},
  address={},
  note={Version 1.0}
}

% Related Software Citation
@software{bloodclarity2025software,
  author={{Macdara Ó Murchú}},
  title={BloodClarity: Clinical Laboratory Report Analysis Platform},
  year={2025},
  url={https://bloodclarity.com},
  version={2.0},
  note={Proprietary medical AI platform (application code not publicly available)}
}

% Dataset Citation
@misc{bloodclarity2025dataset,
  author={{Macdara Ó Murchú}},
  title={Synthetic Laboratory Report Dataset for Biomarker Extraction},
  year={2025},
  howpublished={GitHub Repository},
  url={https://github.com/m4cd4r4/bloodclarity-ml/tree/main/data/synthetic},
  note={10,000 synthetic laboratory reports across 16 formats and 102 biomarkers}
}

% Model Citation
@misc{bloodclarity2025model,
  author={{Macdara Ó Murchú}},
  title={TinyBERT-BloodClarity: Optimized Multi-Task Model for Biomarker Extraction},
  year={2025},
  howpublished={Model Repository},
  url={https://github.com/m4cd4r4/bloodclarity-ml/tree/main/models},
  note={12MB quantized and pruned model, 98.8\% system-level accuracy}
}

% =============================================================================
% Key References Cited in the Paper
% =============================================================================

% Transformer and BERT Models
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  pages={4171--4186},
  year={2019}
}

% Medical NLP Models
@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{alsentzer2019publicly,
  title={Publicly available clinical BERT embeddings},
  author={Alsentzer, Emily and Murphy, John R and Boag, Willie and Weng, Wei-Hung and Jin, Di and Naumann, Tristan and McDermott, Matthew BA},
  journal={arXiv preprint arXiv:1904.03323},
  year={2019}
}

% Model Compression
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS Workshop on Energy Efficient Machine Learning and Cognitive Computing},
  year={2019}
}

@inproceedings{jiao2020tinybert,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4163--4174},
  year={2020}
}

@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}

% Browser-Based ML
@article{smilkov2019tensorflow,
  title={TensorFlow. js: Machine learning for the web and beyond},
  author={Smilkov, Daniel and Thorat, Nikhil and Assogba, Yannick and Yuan, Ann and Kreeger, Nick and Yu, Ping and Zhang, Kangyi and Cai, Shanqing and Nielsen, Eric and Soergel, David and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={309--321},
  year={2019}
}

% Multi-Task Learning
@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

% OCR
@article{otsu1979threshold,
  title={A threshold selection method from gray-level histograms},
  author={Otsu, Nobuyuki},
  journal={IEEE transactions on systems, man, and cybernetics},
  volume={9},
  number={1},
  pages={62--66},
  year={1979},
  publisher={IEEE}
}

@article{levenshtein1966binary,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, Vladimir I},
  journal={Soviet physics doklady},
  volume={10},
  number={8},
  pages={707--710},
  year={1966}
}

% Privacy and Regulations
@misc{hipaa1996,
  title={Health Insurance Portability and Accountability Act of 1996},
  author={{US Congress}},
  howpublished={Public Law 104-191},
  year={1996}
}

@misc{gdpr2016,
  title={General Data Protection Regulation},
  author={{European Parliament and Council}},
  howpublished={Regulation (EU) 2016/679},
  year={2016}
}

% Quantization
@article{krishnamoorthi2018quantizing,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}

@article{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2704--2713},
  year={2018}
}

% Pruning
@inproceedings{frankle2019lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

% Medical AI and LLMs
@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={172--180},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

% =============================================================================
% Usage Instructions
% =============================================================================

% To cite the main paper in your LaTeX document:
%
% \bibliographystyle{plain}
% \bibliography{citation}
%
% Then in your text:
% As demonstrated by \cite{bloodclarity2025clinical}, production-ready accuracy...
%
% For the software:
% The BloodClarity platform \cite{bloodclarity2025software} provides...
%
% For the dataset:
% We trained on the synthetic dataset from \cite{bloodclarity2025dataset}...
